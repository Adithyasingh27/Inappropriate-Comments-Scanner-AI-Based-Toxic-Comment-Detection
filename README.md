# âš ï¸ Inappropriate Comments Scanner  
### AI-Based Toxic Comment Detection System

This project identifies **toxic and inappropriate comments** using an AI-powered text classification model built with **Bidirectional LSTM** and **TensorFlow**. It improves upon traditional keyword-based filtering by understanding **context**, **intent**, and **language patterns**.

---

## ğŸ“Œ Project Overview
Traditional moderation approaches struggle with:
- High manual effort  
- Static keyword rules  
- Poor detection of slang, sarcasm, and context  

This project solves these issues by using a **deep-learning NLP pipeline** capable of real-time classification across multiple toxicity categories.

---

## ğŸš€ Features
âœ”ï¸ Multi-label toxicity detection  
âœ”ï¸ Bidirectional LSTM model for contextual understanding  
âœ”ï¸ End-to-end preprocessing pipeline using `TextVectorization`  
âœ”ï¸ Real-time inference using **Gradio** UI  
âœ”ï¸ Train/Val/Test pipeline using TF Datasets (cache, shuffle, batch, prefetch)  
âœ”ï¸ Exportable `.keras` model  
âœ”ï¸ Scalable design for cloud deployment (AWS/GCP)

---

## ğŸ§  Tech Stack
- **Python**
- **TensorFlow / Keras**
- **Bidirectional LSTM**
- **TextVectorization**
- **Pandas / NumPy**
- **Gradio**
- **MySQL / MongoDB (optional storage)**
- **Cloud Ready** (AWS / GCP)

---

![output1](https://github.com/user-attachments/assets/86e97e61-47a0-4f83-b7e6-1eb62b20e860)
